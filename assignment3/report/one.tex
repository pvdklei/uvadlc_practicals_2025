
\section*{1 Variational Autoencoders}

\subsection*{1.1}

% Question 1.1 (2 points)
% Given a decoder fθ , describe the steps needed to sample an image.

\begin{itemize}
    \item First you have to sample a $\textbf{z}$ from the prior distribution
          $p(\textbf{z})$. So you must sample $\textbf{z} \sim \mathcal{N}(0, I)$.
    \item Then you can pass this $\textbf{z}$ through the decoder $f_{\theta}$ to obtain
          $\textbf{p}$, the parameters of the categorical distribution where every
          $\textbf{p}_m = (p_{m,1}, p_{m,2}, \ldots, p_{m,k})$ represents the
          probabilities of each pixel $m$ taking on each of the $K$ possible intensity
          values.
    \item Finally, you can sample from $p(\textbf{x}|\textbf{z})$ to obtain the generated
          image $\textbf{x}$. Since all pixels are independent random variables, where
          $p(\textbf{x}|\textbf{z}) = \prod_{m} p(\textbf{x}^{(m)}|\textbf{z}) =
              \prod_{m} \text{Categorical}(\textbf{x}^{(m)}|\textbf{p}_m)$ you can sample
          them separately from the categorical distribution. So for each pixel $m$, you
          sample $\textbf{x}^{(m)} \sim \text{Categorical}(\textbf{p}_m)$, and that
          $\textbf{x}_m$ is the pixel value at position $m$.
\end{itemize}

\subsection*{1.2}

% Question 1.2 (3 points)
% Although Monte-Carlo Integration with samples from p(zn) can be used to approximate logp(xn),it is not used for training VAE type of models, because it is inefficient. In a few sentences, describe why it is inefficient and how this efficiency scales with the dimensionality of z. (Hint: you may use Figure 2 in your explanation.)

This is rather inefficient since every time you want to compute $\log
    p(\textbf{x}_n)$, you have to calculate $p(\textbf{x}_n|\textbf{z}_n^{(s)})$
for every sample $\textbf{z}_n^{(s)}$ drawn from the prior. And since we want
to have a good approximation, $L$ has to be quite large. This means that for
every data point $\textbf{x}_n$, we have to do $L$ forward passes through the
decoder network $f_{\theta}$ to compute $p(\textbf{x}_n|\textbf{z}_n^{(s)})$.

This is even more problematic when the dimensionality of $\textbf{z}$
increases, not only because the computation for $f_{\theta}$ becomes more
expensive, but also since then we would need to use an exponentially larger $L$
to get the same approximation quality. An intuitive explanation for this can be
seen in figure 2. If we'd have the same number of dots (samples) in the graph,
but now it would be 3D instead of 2D, the dots would cover a much smaller
portion of the space. The dots would be very sparse and spread out, this not
summarizing the distribution well.

Moreover, in figure 2 we can see that most of the samples of $\textbf{z}$ are
in areas where $p(\textbf{x}_n|\textbf{z})$ is very small. So these samples
contribute very little to the estimate of $p(\textbf{x}_n)$. A lot of samples
are kind off wasted in this way. But when we increase the dimensionality of
$\textbf{z}$, the number of samples that are wasted increase exponentially.
Let's say that over any axis of $\textbf{z}$, only a fraction of $\epsilon$ of
the values contribute significantly to $p(\textbf{x}_n|\textbf{z})$. Then in
$d$ dimensions, only a fraction of $\epsilon^d$ of the samples will contribute
significantly to the estimate of $p(\textbf{x}_n)$. So as $d$ increases, the
number of samples that contribute significantly decreases exponentially,
meaning that we need to increase $L$ exponentially to keep the same estimation
quality.

\subsection*{1.3}

% Question 1.3 (2 points)
% Explain how you can see from Equation 10 that the right hand side has to be a lower bound on the log-probability log p(xn)?

Since the KL-divergence between two distributions is always non-negative, we
know that

\begin{align*}
    \text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n))                                                       & \geq 0                         \\
    \log \, p(\textbf{x}_n) = \text{ELBO}(\textbf{x}_n) + \text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n)) & \geq \text{ELBO}(\textbf{x}_n) \\
    \Rightarrow \log \, p(\textbf{x}_n)                                                                                               & \geq \text{ELBO}(\textbf{x}_n)
\end{align*}

\subsection*{1.4}

% Question 1.4 (2 points)
% Describe what happens to the ELBO as the variational distribution q(zn|xn)
% approaches the true posterior p(zn|xn)?

When the variational distribution $q(\textbf{z}_n|\textbf{x}_n)$ approaches the
true posterior $p(\textbf{z}_n|\textbf{x}_n)$, there are a couple of things
that happen.

First of all, by definition of the KL-divergence,
$\text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n))$
approaches 0.

\begin{align*}
    \text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n)) \to 0
\end{align*}

Because of this, from the equation in question 1.3 we can see that this means
that the ELBO approaches $\log \, p(\textbf{x}_n)$. So as the variational
distribution approaches the true posterior, the ELBO becomes a tighter and
tighter lower bound on the log-probability $\log \, p(\textbf{x}_n)$.

\begin{align*}
    \log \, p(\textbf{x}_n) = \text{ELBO}(\textbf{x}_n) + \text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n)) \\
    \Rightarrow \text{ELBO}(\textbf{x}_n) \to \log \, p(\textbf{x}_n)
\end{align*}

Also, not only does the ELBO approach $\log \, p(\textbf{x}_n)$, but it also
increases in value when the variational distribution approaches the true
posterior.

\begin{align*}
    \text{ELBO}(\textbf{x}_n) = \log \, p(\textbf{x}_n) - \text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n)) \\
    \Rightarrow \text{ELBO}(\textbf{x}_n) \text{ increases}
\end{align*}

But we can't say that the marginal likelihood $\log \, p(\textbf{x}_n)$ will
increase as well

\begin{align*}
    \log \, p(\textbf{x}_n) = \underbrace{\text{ELBO}(\textbf{x}_n)}_{\text{increases}} + \underbrace{\text{D}_{KL}(q(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n|\textbf{x}_n))}_{\text{decreases}}
\end{align*}

\subsection*{1.5}

% Question 1.5 (2 points)
% Explain shortly why the names reconstruction and regularization are appropriate
% for these two losses.
% (Hint: Suppose we use just one sample to approximate the expectation Eqφ(zn|xn)[pθ(xn|zn)] – as is common practice in VAEs.)

\subsubsection*{Reconstruction Loss}

The term reconstruction is appropriate for

\begin{equation*}
    \mathcal{L}^{\text{reconstruction}}_n = - \mathbb{E}_{q_{\phi}(\textbf{z}_n|\textbf{x}_n)}[\log \, p_{\theta}(\textbf{x}_n|\textbf{z}_n)]
\end{equation*}

since it measures how well the decoder is able to reconstruct an output given a
latent variable $\textbf{z}_n$ sampled from the variational distribution
$q_{\phi}(\textbf{z}_n|\textbf{x}_n)$.

If we use just one sample to approximate the expectation, this means that we
sample one $\textbf{z}_n$ from $q_{\phi}(\textbf{z}_n|\textbf{x}_n)$ and then
compute the negative log likelihood $- \log \,
    p_{\theta}(\textbf{x}_n|\textbf{z}_n)$ given this sampled $\textbf{z}_n$.

\begin{align*}
    \mathcal{L}^{\text{reconstruction}}_n & \approx - \log \, p_{\theta}(\textbf{x}_n|\textbf{z}_n) \quad \text{where} \quad \textbf{z}_n \sim q_{\phi}(\textbf{z}_n|\textbf{x}_n) \\
                                          & = - \log \, \text{Categorical}(\textbf{x}_n|\textbf{p}) \quad \text{where} \quad \textbf{p} = f_{\theta}(\textbf{z}_n)
\end{align*}

The way we could minimize this loss is by making sure that the parameters
$\phi$ and $\theta$ are such that when we sample $\textbf{z}_n$ from
$q_{\phi}(\textbf{z}_n|\textbf{x}_n)$ (thus encoding the input image), and then
reconstruct $\textbf{x}_n$ using $p_{\theta}(\textbf{x}_n|\textbf{z}_n)$, the
reconstructed image should have a high probability under the model.

So this reconstruction part makes sure the model is able to encode and decode
the model well, thus justifying the name reconstruction loss, which would also
be used in an autoencoder setting. It simply measures the models ability to
reconstruct the input.

\subsubsection*{Regularization Loss}

The term regularization is appropriate for

\begin{equation*}
    \mathcal{L}^{\text{regularization}}_n = \text{D}_{KL}(q_{\phi}(\textbf{z}_n|\textbf{x}_n) || p(\textbf{z}_n))
\end{equation*}

Since this loss is only dependent on the parameters of the variational
distribution $q_{\phi}(\textbf{z}_n|\textbf{x}_n)$, namely $\phi$. So it cannot
really help with reconstructing the input, since it doesn't have any say over
the decoder parameters $\theta$.

What it does do, is make sure the variational distribution
$q_{\phi}(\textbf{z}_n|\textbf{x}_n)$ is kept similar to a chosen prior
distribution $p(\textbf{z}_n)$, usually a standard normal distribution.

This has a regularizing effect on the model, it doesn't focus on the actual
purpose of the model, but it makes sure the latent space has some desired
properties. Just like in a similar way L2 regularization doesn't focus on
reducing the training error, but it makes sure the weights don't become too
large, thus having a regularizing effect on the model.

\subsection*{1.6}

% Question 1.6 (3 points)
% The above derivation of closed form expression for the regularization term requires Gaussian prior and variational distributions. Assume that we want to model the prior p(z) with a more complex distribution — it is likely the closed form expression would not exist.
% Keeping in mind that DKL(q(z|x)||p(z)) = Ez∼q(z|x)[logq(z|x)], propose an p(z)
% alternative way of estimating the regularization term for a given sample xn.

Since the KL-divergence is essentially just an expectation, we can do a regular
Monte-Carlo estimation of the KL-divergence by sampling $\textbf{z}_n^{(s)}$
from the variational distribution $q_{\phi}(\textbf{z}_n|\textbf{x}_n)$, and
then computing the average of $\log \left(
    \frac{q_{\phi}(\textbf{z}_n^{(s)}|\textbf{x}_n)}{p(\textbf{z}_n^{(s)})}
    \right)$.

\begin{align*}
    \mathcal{L}^{\text{regularization}}_n
     & = \text{D}_{KL}\bigl(q_{\phi}(\textbf{z}_n \mid \textbf{x}_n) \,\|\, p(\textbf{z}_n)\bigr)                                                                                                                                    \\
     & = \mathbb{E}_{q_{\phi}(\textbf{z}_n \mid \textbf{x}_n)}\left[ \log \frac{q_{\phi}(\textbf{z}_n \mid \textbf{x}_n)}{p(\textbf{z}_n)} \right]                                                                                   \\
     & \approx \frac{1}{L} \sum_{s=1}^{L} \left[ \log \frac{q_{\phi}(\textbf{z}_n^{(s)} \mid \textbf{x}_n)}{p(\textbf{z}_n^{(s)})} \right] \quad \text{where} \quad \textbf{z}_n^{(s)} \sim q_{\phi}(\textbf{z}_n \mid \textbf{x}_n)
\end{align*}

This way you don't need a closed from solution for the KL-divergence, you just
estimate it by sampling. Now the only requirement is that you can sample from
the variational distribution and that both this distribution and the prior can
be evaluated to compute the logarithms.

\subsection*{1.7}

% Question 1.7 (2 points)
% Passing the derivative through samples can be done using the reparameterization trick — the process of sampling z directly from N (μ(x), Σ(x)) is commonly replaced by calculating z = Σ(x)ε + μ(x), where ε ∼ N (0, 1). In a few sentences, explain why the act of direct way of sampling usually prevents us from computing ∇φL, and how the reparameterization trick solves this problem.

To update the parameters $\phi$ of the encoder we need to compute the gradient
of the loss with respect to $\phi$, so we need to compute $\frac{\partial
        \mathcal{L}_n}{\partial \phi}$. We'd do this by using the chain rule

\begin{equation*}
    \frac{\partial \mathcal{L}_n}{\partial \phi}
    =
    \frac{\partial \mathcal{L}_n}{\partial \boldsymbol{z}_n}
    \,
    \frac{\partial \boldsymbol{z}_n}{\partial \boldsymbol{\zeta}_n}
    \,
    \frac{\partial \boldsymbol{\zeta}_n}{\partial \phi}.
\end{equation*}

Where $\boldsymbol{\zeta}_n$ are the parameters of the variational distribution
outputted by the encoder. The first and the last term of these equations are
not really special and can be computed normally. The problem lies in the middle
term $\frac{\partial \boldsymbol{z}_n}{\partial \boldsymbol{\zeta}_n}$. Since
$\boldsymbol{z}_n$ is sampled from the distribution parameterized by
$\boldsymbol{\zeta}_n$, this means we have to find the derivative of the
sampling operation?

\begin{equation*}
    \boldsymbol{z}_n = \boldsymbol{z'}_n \quad \text{where} \quad \boldsymbol{z'}_n \sim p(\boldsymbol{\zeta}_n)
\end{equation*}

The sampling operation is as far as I know not a differentiable operation, so
we cannot compute this derivative.

But we can when we use the reparameterization trick. By expressing
$\boldsymbol{z}_n$ as a deterministic function of $\boldsymbol{\zeta}_n$ and
some noise variable $\boldsymbol{\epsilon}$ that is independent of
$\boldsymbol{\zeta}_n$, we canrewrite the sampling operation in a way that
makes it differentiable. For a Gaussian latent variable, we can express
$\boldsymbol{z}_n$ as

\begin{equation*}
    \boldsymbol{z}_n = \boldsymbol{\mu}(\textbf{x}_n) + \boldsymbol{\sigma}(\textbf{x}_n) \odot \boldsymbol{\epsilon}, \quad \text{where} \quad \boldsymbol{\epsilon} \sim \mathcal{N}(0, I).
\end{equation*}

So now we dont have to compute any derivative of a sampling operation, to get
the middle term of the chain rule. We now just have to derivate regular old
additions and multiplications.

\subsection*{1.8}

% Question 1.8 (12 points)
% Build a Variational Autoencoder in the provided templates, and train it on the MNIST dataset. Both the encoder and decoder should be implemented as a CNN. For the architecture, you can use the same as used in Tutorial 9 about Autoencoders. Note that you have to adjust the output shape of the decoder to output 1 × 28 × 28 for MNIST. You can do this by adjusting the output padding of the first transposed convolution in the decoder. Use a latent space size of z_dim=20. Read the provided README to become familiar with the code template.
% In your submission, plot the estimated bit per dimension score of the lower bound on the training and validation set as training progresses, and the final test score. You are allowed to take screenshots of a TensorBoard plot if the axes values are clear.
% Note: using the default hyperparameters is sufficient to obtain full points. As a reference, the training loss should start at around 4 bpd, reach below 2.0 after 2 epochs, and end around 0.52 after 80 epochs.

I got a final test score of 0.5442 bits per dimension. Note that I did not
exactly copy the architecture from tutorial 9, but the question seems to
indicate that this is allowed. Results are similar though.

These are the training (first) and validation (second) bit per dimension plots
from TensorBoard:

\begin{center}
    \includegraphics[width=0.8\textwidth]{imgs/1.8_train_bpd.png}
\end{center}

\begin{center}
    \includegraphics[width=0.8\textwidth]{imgs/1.8_val_bpd.png}
\end{center}

\subsection*{1.9}

% Question 1.9 (3 points)
% Plot 64 samples (8 × 8 grid) from your model at three points throughout training (before training, after training 10 epochs, and after training 80 epochs). You should observe an improvement in the quality of samples. Describe shortly the quality
% and/or issues of the generated images.

Still gotta wait for 1.8 to finish. Will add the images and description as soon
as possible.

\subsection*{1.10}

% Question 1.10 (4 points)
% Train a VAE with a 2-dimensional latent space (z_dim=2 in the code). Use this VAE to plot the data manifold as is done in Figure 4b of [Kingma and Welling, 2014] and was discussed in Lecture 6. This is achieved by taking a two dimensional grid of points in Z-space, and plotting fθ(Z) = μ|Z. Use the percent point function (ppf, or the inverse CDF) to cover the part of Z-space that has significant density. Implement it in the function visualize_manifold in utils.py, and use a grid size of 20. Are you recognizing any patterns of the positions of the different digits?

This is also currently training.

\subsection*{1.11}

% Question 1.11 (Bonus 3 points)
% What do you think will happen to latent representations as we increase β?
% Hint: β-VAEs were first introduced in https: // openreview. net/ pdf? id= Sy2fzU9gl. Their main properties are discussed in page 3 and section 2. Do not worry if you don’t understand all details, it is enough to have a high level idea of what is going on.

I think that increasing $\beta$ will lead to the loss being dominated more by
the regularization term. This will lead to the latent representations being
more similar to the prior distribution, since the regularization term makes
sure the variational distribution is similar to the prior. So, as we increase
$\beta$, the values for $\textbf{z}$ will likely have a mean closer to 0 and a
variance closer to 1, and will have a probability distribution closer to a
standard normal distribution, since that's the prior we are using.

The paper suggests this too, and they give some more reasoning behind it. Since
increasing $\beta$ increases the importance of the KL-divergence term in the
loss, it will force every dimension separately to be more similar to the prior.

\begin{align*}
    \mathcal{L}^{reg} & = \text{D}_{KL}(q_{\phi}(\textbf{z}|\textbf{x}) || p(\textbf{z}))  \\
                      & = \sum_{i=1}^{d} \text{D}_{KL}(q_{\phi}(z_i|\textbf{x}) || p(z_i))
\end{align*}

But when the KL-divergence for some dimension $j$ zero, it also means that that
dimension $z_j$ is completely useless, and that it stores no information about
the input image, because the mutual information between $x$ and $z_j$ is zero,
meaning that $x$ and $z_j$ are independent.

\begin{align*}
    \text{D}_{KL}(q_{\phi}(z_j|\textbf{x}) || p(z_j)) & = 0     \\
    I(x; z_j)                                         & = 0     \\
    z_j                                               & \perp x
\end{align*}

Because of this the authors argue that increasing $\beta$ will force the model
to use a latent representation with a lower total information capacity. So less
information about the image will be stored in the same number of latent
dimensions.

Moreover, their results indicate that the latent dimensions will also have an
uneven information capacity. Where some dimensions will have high $KL(q(z|x) ||
    p(z))$ values (store most information), and others will have very low values
(are useless for reconstructing the image). So in the case that some dimension
$i$ has a high divergence from the prior, then $x$ has a lot of influence on
$z_i$, so that dimension stores a lot of information about the image and has
got a high mutual information $I(x; z_i)$. But when the divergence on some
dimension $j$ is low, for instance zero, then $x$ has no influence on $z_j$,
therefore $I(x; z_j) = 0$ and $x \perp z_j$, so that dimension $z_j$ does not
store any information about the image.

This uneven information spread is likely because the model can't afford to
store the same information in multiple dimensions, i.e., creating dependence
between latent dimensions so that some $z_i \not\!\perp z_j$. Capturing
information in multiple dimensions like this would be suboptimal (you've stored
the same info twice if two dimensions are correlated) use of the limited
information capacity. So since increasing $\beta$ will push the model to use
less information capacity, an easy way to do this is to make sure that the
latent dimensions are independent, so that each dimension has to store unique
information about the image.

So increasing $\beta$ will lead to the latent representations being more
disentangled.. Meaning that different dimensions of the latent space will
represent more cleanly differentiated features of the data, and this can be
seen in the learned representation. So one dimension might represent the more
of the smile of a person, and another dimension might represent the angle of
the head. In our case of MNIST digits, one dimension might represent some
specific aspect of the digit, and another dimension might represent something
else typical for digits. The reasoning behind this is that since you force the
latent representations to be more similar to the prior, and the prior is
independent over the different dimensions, the latent representations will also
become more independent over the different dimensions. Thus giving clearer
roles/functions to all the latent dimensions.

So increasing $\beta$ will penalize the model more for using a suboptimal
encoding strategy, forcing the model to encode the images in a more optimal
way, and the model seems to do this by creating more disentangled/independent
latent representations, and having an uneven/sparser information capacity over
the different latent dimensions. Lastly the authors state that there is an end
to punishing the model for using too much information, since at some point the
model will underfit because it is more important to keep the latent
representations similar to the prior than to reconstruct the image well.

